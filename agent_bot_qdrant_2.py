import json
import sqlite3
import hashlib
import logging
import torch
import config
import threading
import re
import pandas as pd

import telebot
from telebot.util import smart_split

from typing import List, Optional, Sequence, Annotated
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, END, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

from transformers import AutoTokenizer

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchAny
from FlagEmbedding import FlagReranker

import warnings 
warnings.filterwarnings( "ignore", message=".*Local mode is not recommended for collections with more than 20,000 points.*" )
warnings.filterwarnings("ignore", message="You're using a XLMRobertaTokenizerFast tokenizer.*")


# =========================
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# =========================
logging.basicConfig(
    level=logging.INFO,                                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: INFO
    format="%(asctime)s [%(levelname)s] %(message)s",  # –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
    handlers=[logging.StreamHandler()]                 # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–≤–æ–¥–∞ –ª–æ–≥–æ–≤
)


# =========================
# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
# =========================
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑—ã: –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
ABBREVIATIONS_DF = pd.read_excel("documents/abbreviations.xlsx", header=0)

logging.info(f"–ë–∞–∑–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(ABBREVIATIONS_DF)} –∑–∞–ø–∏—Å–µ–π")

def find_abbreviation_expansions(abbreviations: list[str]) -> list[str]:
    """
    –ò—â–µ—Ç –í–°–ï —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –≤ –±–∞–∑–µ ABBREVIATIONS_DF.

    Parameters
    ----------
    abbreviations : list[str]
        –°–ø–∏—Å–æ–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞.

    Returns
    -------
    list[str]
        –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≤–∏–¥–∞ "ABBR: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞".
        –ï—Å–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ ‚Äî –≤—Å–µ –±—É–¥—É—Ç –≤–∫–ª—é—á–µ–Ω—ã.
        –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ ‚Äî –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–∞ "ABBR: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞".
    """
    if not abbreviations:
        return []

    ABBREVIATIONS_DF["abbr"] = ABBREVIATIONS_DF["abbr"].str.upper()

    results = []
    for abbr in abbreviations:
        abbr_upper = abbr.upper()
        matches = ABBREVIATIONS_DF[ABBREVIATIONS_DF["abbr"] == abbr_upper]

        if matches.empty:
            results.append(f"{abbr}: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue

        for _, row in matches.iterrows():
            results.append(f"{abbr}: {row['definition']}")

    return results


# =========================
# –ö–µ—à (SQLite)
# =========================
DB_FILE = "cache.db"

def init_db():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite.
    –°–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—Ü—É 'cache', –µ—Å–ª–∏ –æ–Ω–∞ –µ—â—ë –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    """
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS cache (
            hash TEXT PRIMARY KEY,
            question TEXT,
            answer TEXT
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS messages (
            user_id INTEGER,
            role TEXT CHECK(role IN ('human','assistant')),
            content TEXT,
            ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

def get_hash(text: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA-256 —Ö—ç—à –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    cleaned = re.sub(r"\s+", " ", text).strip().lower()
    return hashlib.sha256(cleaned.encode("utf-8")).hexdigest()

def get_answer(question: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É."""
    h = get_hash(question)
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute("SELECT answer FROM cache WHERE hash=?", (h,))
    row = cur.fetchone()
    conn.close()
    return row[0] if row else None

def add_answer(question: str, answer: str):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."""
    h = get_hash(question)
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute(
        "INSERT OR REPLACE INTO cache (hash, question, answer) VALUES (?, ?, ?)",
        (h, question, answer)
    )
    conn.commit()
    conn.close()

def save_message(user_id: int, role: str, content: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ human/assistant —Å–æ–æ–±—â–µ–Ω–∏—è"""
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute("INSERT INTO messages (user_id, role, content) VALUES (?, ?, ?)", (user_id, role, content))
    conn.commit()
    conn.close()

def load_history(user_id: int, limit: int = 10) -> List[BaseMessage]:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞–≥–µ–Ω—Ç–∞"""
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute("SELECT role, content FROM messages WHERE user_id=? ORDER BY ts DESC LIMIT ?", (user_id, limit))
    rows = cur.fetchall()
    conn.close()

    messages = []  # –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫
    for role, content in reversed(rows):
        if role == "human":
            messages.append(HumanMessage(content=content))
        elif role == "assistant":
            messages.append(AIMessage(content=content))
    return messages


# =========================
# –ú–æ–¥–µ–ª—å LLM –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
# =========================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è LM Studio
LM_STUDIO_URL = "http://127.0.0.1:1234/v1"
MODEL_NAME_LM_STUDIO = "t-lite-it@q4_k_m"

logging.info(f"–ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ LM Studio: {LM_STUDIO_URL}, –º–æ–¥–µ–ª—å {MODEL_NAME_LM_STUDIO}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å OpenAI
llm = ChatOpenAI(
    base_url=LM_STUDIO_URL,
    api_key="lm-studio",              # –ö–ª—é—á –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –Ω–æ –ø–æ–ª–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—É—Å—Ç—ã–º
    model_name=MODEL_NAME_LM_STUDIO,
    temperature=0,                    # –î–ª—è –º–µ–¥–∏—Ü–∏–Ω—ã –ª—É—á—à–µ 0 (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å)
    streaming=False                   # –í–Ω—É—Ç—Ä–∏ LangGraph –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å False –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤
)
tokenizer = AutoTokenizer.from_pretrained("./my_tokenizer")

logging.info("–ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, Qdrant –∏ Reranker
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
client = QdrantClient(path="qdrant_db")
reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True, device='cpu')


# =========================
# –°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞ (Pydantic)
# =========================
class AgentState(BaseModel):
    is_medical_intent: bool = Field(default=True,
        description="–§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ –º–µ–¥–∏—Ü–∏–Ω–µ (True) –∏–ª–∏ –Ω–µ—Ç (False)"
    )
    user_id: Optional[int] = Field(default=None,
        description="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è Telegram –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–µ–π —Å–æ–æ–±—â–µ–Ω–∏–π"
    ) 
    messages: Annotated[Sequence[BaseMessage], add_messages] = Field(default_factory=list,
        description="–ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –¥–∏–∞–ª–æ–≥–∞ (HumanMessage, AIMessage), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"
    )
    rewritten_question: Optional[str] = Field(default=None,
        description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ–±–æ–≥–∞—â–µ–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
    )
    cache_hit: bool = Field(default=False,
        description="–§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –±—ã–ª –ª–∏ –Ω–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –≤ –∫—ç—à–µ (True) –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (False)"
    )
    expanded_queries: List[str] = Field(default_factory=list,
        description="–°–ø–∏—Å–æ–∫ —É—Ç–æ—á–Ω–µ–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º—ã"
    )
    abbreviations: str = Field(default="",
        description="–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏—Ö —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏"
    )
    search_results_text: Optional[str] = Field(default=None,
        description="–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
    )
    answer: Optional[str] = Field(default=None,
        description="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞"
    )


# =========================
# –£–∑–ª—ã –≥—Ä–∞—Ñ–∞
# =========================
def classify_intent_node(state: AgentState) -> AgentState:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–µ–º—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–ª—è (–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π/–Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π)."""
    system_prompt = (
        "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ.\n\n"
        "–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ï –ó–ê–ü–†–û–°–´ –≤–∫–ª—é—á–∞—é—Ç:\n"
            "- —Å–∏–º–ø—Ç–æ–º—ã, –∂–∞–ª–æ–±—ã, —Å–∏–Ω–¥—Ä–æ–º—ã\n"
            "- –¥–∏–∞–≥–Ω–æ–∑—ã, –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è, –Ω–æ–∑–æ–ª–æ–≥–∏–∏\n"
            "- –∞–Ω–∞–ª–∏–∑—ã, –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É\n"
            "- –ª–µ–∫–∞—Ä—Å—Ç–≤–∞, –¥–æ–∑–∏—Ä–æ–≤–∫–∏, —Å—Ö–µ–º—ã –ª–µ—á–µ–Ω–∏—è\n"
            "- –æ–ø–µ—Ä–∞—Ü–∏–∏, –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, —Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—é\n"
            "- –∞–Ω–∞—Ç–æ–º–∏—é, —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—é, –ø–∞—Ç–æ—Ñ–∏–∑–∏–æ–ª–æ–≥–∏—é\n"
            "- –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø—Ä–∏–±–æ—Ä—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
            "- –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ê–ì, –•–°–ù, –°–î2 –∏ —Ç.–¥.)\n\n"

        "–û–ë–©–ò–ï –ó–ê–ü–†–û–°–´ –≤–∫–ª—é—á–∞—é—Ç:\n"
            "- –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, small talk\n"
            "- –≤–æ–ø—Ä–æ—Å—ã –æ –ª–∏—á–Ω–æ—Å—Ç–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n"
            "- –±—ã—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —é–º–æ—Ä, —Ñ–∏–ª–æ—Å–æ—Ñ–∏—é\n"
            "- —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
            "- –ª—é–±—ã–µ —Ç–µ–º—ã, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –º–µ–¥–∏—Ü–∏–Ω–æ–π\n\n"

        "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ:\n"
        "{\"is_medical_intent\": true} –∏–ª–∏ {\"is_medical_intent\": false}"
    )
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=(
            f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{"\n".join([m.content for m in state.messages[:-1]])}\n\n"
            f"–ü–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ç—Ä–µ–±—É—é—â–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n{state.messages[-1].content}"
        ))
    ]
    response = llm.invoke(messages).content
    
    try:
        match = re.search(r"\{.*\}", response, re.DOTALL).group(0)       # –∏—â–µ–º –ø–µ—Ä–≤—ã–π JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        intent = json.loads(match)                                       # –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º JSON-—Å—Ç—Ä–æ–∫—É –≤ Python-—Å–ø–∏—Å–æ–∫
        state.is_medical_intent = intent.get("is_medical_intent", True)  # –ø–æ–ª—É—á–∞–µ–º –º–µ–Ω–∫—É –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ç LLM

        logging.info(f"–£–∑–µ–ª (classify_intent_node). –û–±–Ω–∞—Ä—É–∂–µ–Ω {"–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π" if state.is_medical_intent else "–Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π"} –∑–∞–ø—Ä–æ—Å.")

    except Exception as e:
        logging.error(f"–£–∑–µ–ª (classify_intent_node). –û—à–∏–±–∫–∞: {e}")
        state.is_medical_intent = True # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º
    
    torch.cuda.empty_cache()
    return state

def detect_abbreviations_node(state: AgentState) -> AgentState:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –≤ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    system_prompt = (
        "–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –∏ –≤—ã–¥–µ–ª–∏ –í–°–ï –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è.\n\n"
        "–ò–ù–°–¢–†–£–ö–¶–ò–ò:\n"
        "- –í—ã–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: –û–†–í–ò, –•–ë–ü, –°–î, –ê–ì, –ò–ë–° –∏ –¥—Ä—É–≥–∏–µ)\n"
        "- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –æ–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä: –∏ —Ç.–¥., –∏ –¥—Ä.)\n"
        "- –í–∫–ª—é—á–∞–π –∫–∞–∫ —Ä—É—Å—Å–∫–∏–µ, —Ç–∞–∫ –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã\n"
        "- –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –Ω–µ—Ç - –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π JSON –º–∞—Å—Å–∏–≤\n"
        "- –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä: [\"–û–†–í–ò\", \"–ê–ì\", \"–ò–ë–°\"]\n\n"

        "–ü–†–ò–ú–ï–†–´:\n"
        "–í–æ–ø—Ä–æ—Å: '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ê–ì –∏ –ª–µ—á–µ–Ω–∏–µ –°–î 2 —Ç–∏–ø–∞' ‚Üí [\"–ê–ì\", \"–°–î\"]\n"
        "–í–æ–ø—Ä–æ—Å: '–∫–∞–∫ –ª–µ—á–∏—Ç—å –û–†–í–ò —É –¥–µ—Ç–µ–π?' ‚Üí [\"–û–†–í–ò\"]\n"
        "–í–æ–ø—Ä–æ—Å: '—á—Ç–æ —Ç–∞–∫–æ–µ –•–ë–ü 3 —Å—Ç–∞–¥–∏–∏?' ‚Üí [\"–•–ë–ü\"]\n"
        "–í–æ–ø—Ä–æ—Å: '–æ–±—ã—á–Ω–∞—è –ø—Ä–æ—Å—Ç—É–¥–∞' ‚Üí []"
    )
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=state.messages[-1].content)
    ]
    response = llm.invoke(messages).content
    
    try:
        match = re.search(r"\[.*?\]", response, re.DOTALL).group(0)                  # –∏—â–µ–º –ø–µ—Ä–≤—ã–π JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        abbreviations = json.loads(match)                                            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º JSON-—Å—Ç—Ä–æ–∫—É –≤ Python-—Å–ø–∏—Å–æ–∫
        abbreviations_with_expansions = find_abbreviation_expansions(abbreviations)  # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–æ–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å
        state.abbreviations = "–†–ê–°–®–ò–§–†–û–í–ö–ò –ê–ë–ë–†–ï–í–ò–ê–¢–£–†:\n" + "\n".join(abbreviations_with_expansions)

        logging.info(f"–£–∑–µ–ª (detect_abbreviations_node). –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: {abbreviations_with_expansions}")

    except Exception as e:
        logging.error(f"–£–∑–µ–ª (detect_abbreviations_node). –û—à–∏–±–∫–∞: {e}")

    torch.cuda.empty_cache()
    return state

def rewrite_node(state: AgentState) -> AgentState:
    """–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä."""
    system_prompt = (
        "–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –µ–≥–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö.\n\n"
        "–ü–ï–†–ï–§–û–†–ú–£–õ–ò–†–£–ô –ï–°–õ–ò:\n"
            "- –í–æ–ø—Ä–æ—Å –∫–æ—Ä–æ—á–µ 3 —Å–ª–æ–≤ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π\n"
            "- –ï—Å—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ/–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏\n"
            "- –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
            "- –ï—Å—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö\n\n"

        "–ù–ï –ú–ï–ù–Ø–ô –ï–°–õ–ò:\n"
            "- –í–æ–ø—Ä–æ—Å —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã\n"
            "- –ß–µ—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n"
            "- –ù–µ –∫–∞—Å–∞–µ—Ç—Å—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏\n\n"

        "–ü–†–ê–í–ò–õ–ê:\n"
            "- –°–æ—Ö—Ä–∞–Ω—è–π –∏—Å—Ö–æ–¥–Ω—ã–π —Å–º—ã—Å–ª, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–º—ã—Å–ª.\n"
            "- –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã.\n"
            "- –î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–∏–º–ø—Ç–æ–º—ã, –≤–æ–∑—Ä–∞—Å—Ç, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ).\n"
            "- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.\n"
            "- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ö–æ—Ä–æ—à–∏–π - –≤–µ—Ä–Ω–∏ –µ–≥–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π."
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞—Ö, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    if state.abbreviations:
        system_prompt += f"\n\n–í–û–ó–ú–û–ñ–ù–´–ï –†–ê–°–®–ò–§–†–û–í–ö–ò –ê–ë–ë–†–ï–í–ò–ê–¢–£–†, –ù–ê–ô–î–ï–ù–ù–´–• –í –í–û–ü–†–û–°–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{state.abbreviations}."

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=(
            f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{"\n".join([m.content for m in state.messages[:-1]])}\n\n"
            f"–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{state.messages[-1].content}"))
    ]
    # input_tokens = len(tokenizer.encode("\n".join([m.content for m in state.messages[:-1]]) + state.messages[-1].content + system_prompt))
    # logging.info(f"–£–∑–µ–ª (rewrite_node). –ù–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–Ω–æ {input_tokens} ({input_tokens*100/16384:.2f}%) —Ç–æ–∫–µ–Ω–æ–≤.")

    response = llm.invoke(messages).content
    state.rewritten_question = response

    logging.info(f"–£–∑–µ–ª (rewrite_node). –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞: '{state.messages[-1].content}' -> '{response}'.")
    
    torch.cuda.empty_cache()
    return state

def check_cache_node(state: AgentState) -> AgentState:
    """–ü–æ–∏—Å–∫ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∫—ç—à–µ."""
    cached = get_answer(state.rewritten_question)
    if cached:
        state.answer = cached
        state.cache_hit = True
    else:
        state.cache_hit = False
    return state

def expand_node(state: AgentState) -> AgentState:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö."""
    max_questions = 3
    system_prompt = (
        "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.\n\n"

        "–¶–ï–õ–¨: –ù–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –±–∞–∑–µ\n\n"

        "–ü–†–ê–í–ò–õ–ê –°–û–ó–î–ê–ù–ò–Ø –ó–ê–ü–†–û–°–û–í:\n"
            f"- –ü–æ–¥–≥–æ—Ç–æ–≤—å –¥–æ {max_questions} —É—Ç–æ—á–Ω—è—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n"
            "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã\n"
            "- –í–∫–ª—é—á–∞–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ê–ì ‚Üí –∞—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è)\n"
            "- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö, –∞ –Ω–µ –Ω–∞ –ø–æ–ª–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö\n"
            "- –†–∞–∑–¥–µ–ª—è–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã\n"
            "- –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ–∑–æ–ª–æ–≥–∏–∏, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –ø—Ä–æ—Ü–µ–¥—É—Ä—ã\n"
            "- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n\n"

        "–ß–ï–ì–û –ò–ó–ë–ï–ì–ê–¢–¨:\n"
            "- –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º ('–∫–∞–∫ –ª–µ—á–∏—Ç—å?' ‚Üí '–ª–µ—á–µ–Ω–∏–µ')\n"
            "- –û–±—â–∏—Ö —Ñ—Ä–∞–∑ ('—á—Ç–æ –¥–µ–ª–∞—Ç—å –ø—Ä–∏' ‚Üí –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)\n"
            "- –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π\n"
            "- –õ–∏—à–Ω–∏—Ö —Å–ª–æ–≤\n\n"

        "–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫ –∏ –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π, —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
        "[\"–≤–æ–ø—Ä–æ—Å1\", \"–≤–æ–ø—Ä–æ—Å2\", \"–≤–æ–ø—Ä–æ—Å3\"]"
    )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=state.rewritten_question)
    ]
    response = llm.invoke(messages).content

    try:
        match = re.search(r"\[.*?\]", response, re.DOTALL).group(0)  # –∏—â–µ–º –ø–µ—Ä–≤—ã–π JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        queries = json.loads(match)[:max_questions]                  # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º JSON-—Å—Ç—Ä–æ–∫—É –≤ Python-—Å–ø–∏—Å–æ–∫
        state.expanded_queries = queries or [state.rewritten_question]

        logging.info(f"–£–∑–µ–ª (expand_node). –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤.")

    except Exception as e:
        queries = []
        logging.error(f"–£–∑–µ–ª (expand_node). –û—à–∏–±–∫–∞: {e}")
    
    torch.cuda.empty_cache()
    return state

def retrieve_node(state: AgentState) -> AgentState:
    """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ Qdrant —Å —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º BGE."""
    k_docs = 3        # –°–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ö–†) –∏—Å–∫–∞—Ç—å –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ
    k_chunks = 100    # –°–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏—Å–∫–∞—Ç—å –≤–Ω—É—Ç—Ä–∏ —ç—Ç–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    max_tokens = 1200
    
    final_blocks = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    for query in state.expanded_queries:

        logging.info(f"–£–∑–µ–ª (retrieve_node). –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å --> {query}")

        query_vector = embeddings.embed_query(f"query: {query}")

        # –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Parent Search)
        search_docs = client.query_points(
            collection_name="clin_rec_docs", 
            query=query_vector, 
            limit=k_docs
        ).points

        found_doc_ids = [str(d.payload['doc_id']) for d in search_docs]  # –ò–∑–≤–ª–µ–∫–∞–µ–º doc_id –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        logging.info(f"–£–∑–µ–ª (retrieve_node). –í—ã—Ä–±–∞–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã: {[d.payload['name'][:30] for d in search_docs]}")

        # –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Child Search)
        search_chunks = client.query_points(
            collection_name="clin_rec_chunks",
            query=query_vector,
            limit=k_chunks,
            query_filter=Filter(must=[FieldCondition(key="metadata.doc_id", match=MatchAny(any=found_doc_ids))])
        ).points

        # –≠–¢–ê–ü 3: –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï (Cross-Encoding)
        pairs = []  # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä—ã [–≤–æ–ø—Ä–æ—Å, —Ç–µ–∫—Å—Ç_—á–∞–Ω–∫–∞]
        for hit in search_chunks:
            content = hit.payload.get('page_content', hit.payload.get('metadata', {}).get('page_content', ""))
            pairs.append([query, content])
        scores = reranker.compute_score(pairs)  # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç BGE –º–æ–¥–µ–ª–∏
    
        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ —á–∞–Ω–∫–∞–º
        for hit, score in zip(search_chunks, scores):
            hit.score = score # –ó–∞–º–µ–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∫–æ—Å–∏–Ω—É—Å –Ω–∞ –æ—Ü–µ–Ω–∫—É —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
        search_chunks.sort(key=lambda x: x.score, reverse=True)

        # –≠–¢–ê–ü 4: –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        current_tokens = 0
        for chunk in search_chunks:
            text = f"–ò—Å—Ç–æ—á–Ω–∏–∫ [{chunk.payload.get('metadata', {}).get('name', "")}]: {chunk.payload.get("page_content", "")[9:]}"
            chunk_tokens = len(tokenizer.encode(text))
  
            final_blocks.append(text)
            current_tokens += chunk_tokens

            if current_tokens + chunk_tokens > max_tokens:
                break
    
    state.search_results_text = "\n\n".join(final_blocks)

    logging.info(f"–£–∑–µ–ª (retrieve_node). –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(tokenizer.encode(state.search_results_text))} —Ç–æ–∫–µ–Ω–æ–≤.")
    
    return state

def medical_generate_node(state: AgentState) -> AgentState:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    system_prompt = (
        "–¢—ã - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–º–æ—â–∏ –≤—Ä–∞—á–∞–º. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n\n"

        "–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:\n"
            "–î–ï–õ–ê–ô: –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n"
            "–î–ï–õ–ê–ô: –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–æ–∑–∏—Ä–æ–≤–∫–∏, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n"
            "–î–ï–õ–ê–ô: –£–∫–∞–∑—ã–≤–∞–π —á–µ—Ç–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–µ–π—Å—Ç–≤–∏–π –ø–æ —à–∞–≥–∞–º\n"
            "–î–ï–õ–ê–ô: –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏\n"
            "–î–ï–õ–ê–ô: –£–∫–∞–∑—ã–≤–∞–π —Ç–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤ –∏ —Å—Ö–µ–º—ã –ª–µ—á–µ–Ω–∏—è\n"
            "–ù–ï–õ–¨–ó–Ø: –î–æ–±–∞–≤–ª—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–µ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "–ù–ï–õ–¨–ó–Ø: –î–∞–≤–∞—Ç—å –ª–∏—á–Ω—ã–µ –º–Ω–µ–Ω–∏—è –∏–ª–∏ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n"
            "–ù–ï–õ–¨–ó–Ø: –ù–ï –ø—Ä–µ–¥–ª–∞–≥–∞–π –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –≤—Ä–∞—á—É (—ç—Ç–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç—Å—è)\n"
            "–ù–ï–õ–¨–ó–Ø: –°—Ç–∞–≤–∏—Ç—å –¥–∏–∞–≥–Ω–æ–∑—ã –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n"

        f"–ò–ù–§–û–†–ú–ê–¶–ò–Ø –î–õ–Ø –û–¢–í–ï–¢–ê –ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–• –ö–õ–ò–ù–ò–ß–ï–°–ö–ò–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:\n{state.search_results_text}\n\n"
        "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."
    )
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=state.rewritten_question)
    ]
    
    input_tokens = len(tokenizer.encode(state.rewritten_question + system_prompt))
    logging.info(f"–£–∑–µ–ª (medical_generate_node). –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ù–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–Ω–æ {input_tokens} ({input_tokens*100/16384:.2f}%) —Ç–æ–∫–µ–Ω–æ–≤.")

    state.answer = llm.invoke(messages).content
    
    output_tokens = len(tokenizer.encode(state.answer))
    logging.info(f"–£–∑–µ–ª (medical_generate_node). –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ —Å–æ—Å—Ç–∞–≤–∏–ª {output_tokens} ({output_tokens*100/16384:.2f}%) —Ç–æ–∫–µ–Ω–æ–≤.")

    torch.cuda.empty_cache()
    return state

def no_medical_generate_node(state: AgentState) -> AgentState:
    system_prompt = (
        "–¢—ã - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–º–æ—â–∏ –≤—Ä–∞—á–∞–º. "
        "–ù–∞ –Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ—Ç–≤–µ—á–∞–π –≤–µ–∂–ª–∏–≤–æ, –Ω–æ –∫—Ä–∞—Ç–∫–æ. –ï—Å–ª–∏ —Ç–µ–±—è –ø—Ä–æ—Å—è—Ç —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ-—Ç–æ –Ω–µ –ø–æ —Ç–µ–º–µ "
        "(–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥), –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å, —Å–∫–∞–∑–∞–≤, —á—Ç–æ —Ç—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω–µ."
    )
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=(
            f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{"\n".join([m.content for m in state.messages[:-1]])}\n\n"
            f"–ü–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{state.messages[-1].content}"
        ))
    ]

    input_tokens = len(tokenizer.encode(state.messages[-1].content + system_prompt))
    logging.info(f"–£–∑–µ–ª (no_medical_generate_node). –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ù–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–Ω–æ {input_tokens} ({input_tokens*100/16384:.2f}%) —Ç–æ–∫–µ–Ω–æ–≤.")

    state.answer = llm.invoke(messages).content

    output_tokens = len(tokenizer.encode(state.answer))
    logging.info(f"–£–∑–µ–ª (no_medical_generate_node). –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ —Å–æ—Å—Ç–∞–≤–∏–ª {output_tokens} ({output_tokens*100/16384:.2f}%) —Ç–æ–∫–µ–Ω–æ–≤.")

    torch.cuda.empty_cache()
    return state

def save_cache_node(state: AgentState) -> AgentState:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞ –≤ –∫—ç—à–µ, –µ—Å–ª–∏ –æ–Ω –Ω–æ–≤—ã–π (–Ω–µ –Ω–∞–π–¥–µ–Ω —Ä–∞–Ω–µ–µ)."""
    add_answer(state.rewritten_question, state.answer)
    logging.info(f"–£–∑–µ–ª (save_cache_node). –û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫—ç—à–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: {state.rewritten_question}")
    return state


# =========================
# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
# =========================
graph = StateGraph(AgentState)

graph.set_entry_point("classify_intent")  # –ù–∞—á–∏–Ω–∞–µ–º —Å —É–∑–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º—ã —Å–æ–æ–±—â–µ–Ω–∏—è

graph.add_node("classify_intent", classify_intent_node)
graph.add_node("detect_abbreviations", detect_abbreviations_node)
graph.add_node("rewrite", rewrite_node)
graph.add_node("check_cache", check_cache_node)
graph.add_node("expand", expand_node)
graph.add_node("retrieve", retrieve_node)
graph.add_node("medical_generate", medical_generate_node)
graph.add_node("save_cache", save_cache_node)
graph.add_node("no_medical_generate", no_medical_generate_node)

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ª–æ–≥–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º—ã —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–µ—Ç–∫–∏
def route_intent(state: AgentState):
    if state.is_medical_intent:  # –í–µ—Ç–≤–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø–æ –º–µ–¥–∏—Ü–∏–Ω–µ ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –≤–µ—Ç–∫–∞, –∏–Ω–∞—á–µ ‚Äî –Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –≤–µ—Ç–∫–∞
        return "detect_abbreviations"
    return "no_medical_generate"

graph.add_conditional_edges("classify_intent", route_intent,
    {
        "detect_abbreviations": "detect_abbreviations",
        "no_medical_generate": "no_medical_generate"
    }
)
graph.add_edge("no_medical_generate", END)  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–µ–º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –≤–µ—Ç–∫—É

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ª–æ–≥–∏–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–µ—Ç–∫–∏
graph.add_edge("detect_abbreviations", "rewrite")
graph.add_edge("rewrite", "check_cache")

def route_after_cache(state: AgentState):
    return END if state.cache_hit else "expand"  # –í–µ—Ç–≤–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ –∫–µ—à –Ω–∞–π–¥–µ–Ω ‚Äî –∫–æ–Ω–µ—Ü, –∏–Ω–∞—á–µ –∏–¥—ë–º –¥–∞–ª—å—à–µ

graph.add_conditional_edges("check_cache", route_after_cache, 
    {
        "expand": "expand", 
        END: END  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –≤–µ—Ç–∫—É, –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞
    }
)

graph.add_edge("expand", "retrieve")
graph.add_edge("retrieve", "medical_generate")
graph.add_edge("medical_generate", "save_cache")
graph.add_edge("save_cache", END)

memory = MemorySaver()
app = graph.compile(checkpointer=memory)


# =========================
# Telegram bot —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º
# =========================
bot = telebot.TeleBot(config.TOKEN)

# –°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—á–µ—Ä–µ–¥—å—é –∑–∞–ø—Ä–æ—Å–æ–≤
lock = threading.Lock()
busy = False
current_user = None

@bot.message_handler(commands=['start'])
def welcome(message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    bot.send_message(message.chat.id, "üëã –ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ë–∏–Ω—Ç–∏–∫.")

@bot.message_handler(commands=['status'])
def status(message):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –±–æ—Ç–∞"""
    with lock:
        status_text = f"ü§ñ –°—Ç–∞—Ç—É—Å –±–æ—Ç–∞: {'–ó–∞–Ω—è—Ç' if busy else '–°–≤–æ–±–æ–¥–µ–Ω'}"
        if busy:
            status_text += f"\n–¢–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {current_user}"
        bot.send_message(message.chat.id, status_text)

@bot.message_handler(content_types=['text'])
def handle_query(message):
    """–û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º"""
    global busy, current_user
    user_id = message.chat.id
    user_question = message.text
    logging.info(f"–ó–∞–ø—Ä–æ—Å –æ—Ç {user_id}: {user_question}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –±–æ—Ç–∞
    with lock:
        if busy and current_user != user_id:
            bot.send_message(user_id, "‚ö†Ô∏è –ó–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –∑–∞–ø—Ä–æ—Å–æ–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
            return
        busy = True
        current_user = user_id

    try:
        bot.send_message(user_id, "üí≠ –ó–∞–¥–∞—á—É –ø—Ä–∏–Ω—è–ª, —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")

        final_state: AgentState = app.invoke(
            {"user_id": user_id, "messages": load_history(user_id, limit=3) + [HumanMessage(content=user_question)]},
            config={"configurable": {"thread_id": str(user_id)}}
        )

        answer = final_state.get("answer")  # –û—Ç–≤–µ—Ç –ê–≥–µ–Ω—Ç–∞

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—É—é
        for chunk in smart_split(answer, chars_per_string=4000):
            bot.send_message(user_id, chunk)

        save_message(user_id, "human", user_question)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –±–∞–∑—É SQL
        save_message(user_id, "assistant", answer)     # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ –±–∞–∑—É SQL

    except Exception as e:
        bot.send_message(user_id, f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º.\n{e}")
        logging.error(f"–û—à–∏–±–∫–∞ {e}")

    finally:
        with lock:
            busy = False
            current_user = None

if __name__ == "__main__":
    logging.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∫–µ—à–∞...")
    init_db()
    logging.info("–ó–∞–ø—É—Å–∫ Telegram-–±–æ—Ç–∞...")
    bot.polling(non_stop=True)
